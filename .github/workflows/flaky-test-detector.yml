name: Flaky Test Detector

on:
  workflow_run:
    workflows: ["CI"]  # Replace with your test workflow name
    types:
      - completed

jobs:
  detect-flaky-tests:
    runs-on: ubuntu-latest
    # Only run if the workflow failed
    if: ${{ github.event.workflow_run.conclusion == 'failure' }}

    permissions:
      contents: read
      pull-requests: write
      issues: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history for git diff comparisons

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install runpod requests

      - name: Get last successful run
        id: last-success
        uses: actions/github-script@v7
        with:
          script: |
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'ci.yml',
              branch: context.payload.workflow_run.head_branch,
              status: 'success',
              per_page: 1
            });

            if (runs.data.workflow_runs.length > 0) {
              const lastSuccess = runs.data.workflow_runs[0];
              core.setOutput('sha', lastSuccess.head_sha);
              core.setOutput('run_number', lastSuccess.run_number);
            } else {
              core.setOutput('sha', '');
              core.setOutput('run_number', '');
            }

      - name: Analyze code changes
        id: changes
        if: steps.last-success.outputs.sha != ''
        continue-on-error: true
        run: |
          LAST_SHA="${{ steps.last-success.outputs.sha }}"
          CURRENT_SHA="${{ github.event.workflow_run.head_sha }}"

          # Verify the commit exists
          if ! git cat-file -e $LAST_SHA^{commit} 2>/dev/null; then
            echo "Warning: Last successful commit $LAST_SHA not found in history"
            echo "total_changed=0" >> $GITHUB_OUTPUT
            echo "python_changed=0" >> $GITHUB_OUTPUT
            echo "commit_count=0" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Get changed files
          git diff --name-only $LAST_SHA $CURRENT_SHA > changed_files.txt

          # Get statistics
          TOTAL_CHANGED=$(wc -l < changed_files.txt | tr -d ' ')
          PYTHON_CHANGED=$(grep "\.py$" changed_files.txt | wc -l | tr -d ' ')

          # Get detailed diff
          git diff --stat $LAST_SHA $CURRENT_SHA > diff_stats.txt

          # Get commit information
          git log --format="%h|%an|%ar|%s" $LAST_SHA..$CURRENT_SHA > commits_detailed.txt
          COMMIT_COUNT=$(git rev-list --count $LAST_SHA..$CURRENT_SHA)

          echo "total_changed=$TOTAL_CHANGED" >> $GITHUB_OUTPUT
          echo "python_changed=$PYTHON_CHANGED" >> $GITHUB_OUTPUT
          echo "last_success_sha=$LAST_SHA" >> $GITHUB_OUTPUT
          echo "commit_count=$COMMIT_COUNT" >> $GITHUB_OUTPUT

      - name: Extract failed test information
        id: extract-tests
        run: |
          # Get the workflow run logs and extract failed tests
          # This is a simplified version - adjust based on your test framework
          echo "failed_tests=pytest tests/" >> $GITHUB_OUTPUT
          echo "pr_number=${{ github.event.workflow_run.pull_requests[0].number }}" >> $GITHUB_OUTPUT

      - name: Run flaky test detector
        id: flaky-detector
        env:
          RUNPOD_API_KEY: ${{ secrets.RUNPOD_API_KEY }}
          RUNPOD_ENDPOINT_ID: ${{ secrets.RUNPOD_ENDPOINT_ID }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          TEST_COMMAND: ${{ steps.extract-tests.outputs.failed_tests }}
          LAST_SUCCESS_SHA: ${{ steps.changes.outputs.last_success_sha }}
          CURRENT_SHA: ${{ github.event.workflow_run.head_sha }}
        run: |
          python << 'EOF'
          import runpod
          import os
          import json

          runpod.api_key = os.environ['RUNPOD_API_KEY']
          endpoint = runpod.Endpoint(os.environ['RUNPOD_ENDPOINT_ID'])

          # Run flaky test detection
          job = endpoint.run({
              "repo": f"https://github.com/{os.environ['GITHUB_REPOSITORY']}",
              "test_command": os.environ['TEST_COMMAND'],
              "runs": 100,
              "parallelism": 10
          })

          result = job.output()

          # Save results to file
          with open('flaky_test_results.json', 'w') as f:
              json.dump(result, f, indent=2)

          # Determine severity
          rate = result['repro_rate']
          if rate >= 0.9:
              severity = "ðŸ”´ CRITICAL"
              severity_level = "CRITICAL"
          elif rate >= 0.5:
              severity = "ðŸŸ  HIGH"
              severity_level = "HIGH"
          elif rate >= 0.1:
              severity = "ðŸŸ¡ MEDIUM"
              severity_level = "MEDIUM"
          elif rate > 0:
              severity = "ðŸŸ¢ LOW"
              severity_level = "LOW"
          else:
              severity = "âœ… NONE"
              severity_level = "NONE"

          # Write to GITHUB_OUTPUT
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"severity={severity}\n")
              f.write(f"severity_level={severity_level}\n")
              f.write(f"repro_rate={rate}\n")
              f.write(f"total_runs={result['total_runs']}\n")
              f.write(f"failures={result['failures']}\n")

          # Generate summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write(f'## {severity} Flaky Test Detection Results\n\n')
              f.write(f'**Severity:** {severity_level}\n\n')

              # Show changes since last success
              last_success_sha = os.environ.get('LAST_SUCCESS_SHA', '')
              if last_success_sha and os.path.exists('changed_files.txt'):
                  f.write('### ðŸ“ Changes That May Have Introduced Flakiness\n\n')

                  with open('changed_files.txt') as cf:
                      changed_files = [line.strip() for line in cf if line.strip()]

                  python_files = [f for f in changed_files if f.endswith('.py')]
                  test_files = [f for f in changed_files if 'test' in f.lower() and f.endswith('.py')]

                  current_sha = os.environ.get("CURRENT_SHA", "HEAD")

                  # Show commit information
                  if os.path.exists('commits_detailed.txt'):
                      with open('commits_detailed.txt') as cm:
                          commits = [line.strip() for line in cm if line.strip()]

                      f.write(f'**Commits:** {len(commits)} new commit(s)\n')
                      f.write(f'**Comparing:** `{last_success_sha[:7]}...{current_sha[:7]}`\n\n')

                      if commits:
                          f.write('**Commits in this change:**\n')
                          for commit in commits[:5]:
                              parts = commit.split('|', 3)
                              if len(parts) >= 4:
                                  sha, author, time, message = parts[0], parts[1], parts[2], parts[3]
                                  # Truncate message
                                  if len(message) > 50:
                                      message = message[:47] + '...'
                                  f.write(f'- `{sha}` {message} _{author} ({time})_\n')
                          if len(commits) > 5:
                              f.write(f'- ... and {len(commits) - 5} more commits\n')
                          f.write('\n')
                  else:
                      f.write(f'**Comparing:** `{last_success_sha[:7]}...{current_sha[:7]}`\n\n')

                  if python_files:
                      f.write('**Python Files Changed:**\n')
                      for pf in python_files[:8]:
                          f.write(f'- `{pf}`\n')
                      if len(python_files) > 8:
                          f.write(f'- ... and {len(python_files) - 8} more\n')
                      f.write('\n')

                  # Highlight potentially problematic changes
                  core_files = [f for f in python_files if f in ['worker.py', 'config.py', 'database.py']]
                  if core_files:
                      f.write('âš ï¸ **Core modules modified:** ')
                      f.write(', '.join([f'`{f}`' for f in core_files]))
                      f.write('\n\n')

              # Statistics
              f.write('### Summary\n\n')
              f.write('| Metric | Value |\n')
              f.write('|--------|-------|\n')
              f.write(f"| Total Runs | {result['total_runs']} |\n")
              f.write(f"| Parallelism | {result['parallelism']} |\n")
              f.write(f"| âœ… Passed | {result['total_runs'] - result['failures']} |\n")
              f.write(f"| âŒ Failed | {result['failures']} |\n")
              f.write(f"| ðŸ“Š Failure Rate | {rate * 100:.1f}% |\n\n")

              # Interpretation
              f.write('### Analysis\n\n')
              if rate >= 0.9:
                  f.write('âŒ **This appears to be a real bug**, not flaky behavior. '
                         'The test fails consistently (â‰¥90% of the time).\n\n')
                  f.write('**Recommendation:** Fix the underlying issue before merging.\n')
              elif rate >= 0.5:
                  f.write('âš ï¸ **Very unstable test.** This test fails more than half the time, '
                         'indicating severe flakiness.\n\n')
                  f.write('**Recommendation:** Block this PR and fix the flaky test.\n')
              elif rate >= 0.1:
                  f.write('ðŸŸ¡ **Clear flaky behavior.** This test fails intermittently '
                         'but frequently enough to be problematic.\n\n')
                  f.write('**Recommendation:** Should be fixed before merging.\n')
              elif rate > 0:
                  f.write('ðŸŸ¢ **Low-level flakiness.** This test occasionally fails '
                         'but is mostly stable.\n\n')
                  f.write('**Recommendation:** Monitor and consider fixing when time allows.\n')
              else:
                  f.write('âœ… **No flaky behavior detected.** The test failure appears '
                         'to be a one-time issue, possibly environmental.\n\n')
                  f.write('**Recommendation:** Safe to retry or merge if tests pass on retry.\n')

              # Failed runs sample
              if result['failures'] > 0:
                  f.write('\n### Sample Failed Runs\n\n')
                  failed_runs = [r for r in result['results'] if not r['passed']][:5]
                  for i, run in enumerate(failed_runs, 1):
                      f.write(f'**Run #{run["attempt"]}:**\n')
                      if run.get('stderr'):
                          error_msg = run['stderr'][:200]
                          f.write(f'```\n{error_msg}\n```\n\n')
                      elif run.get('stdout'):
                          output_msg = run['stdout'][:200]
                          f.write(f'```\n{output_msg}\n```\n\n')

              f.write('\n---\n\n')
              f.write('*Detected by Serverless Flaky Test Detector*\n')
          EOF

      - name: Post results to PR
        if: steps.extract-tests.outputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const result = JSON.parse(fs.readFileSync('flaky_test_results.json', 'utf8'));

            const rate = result.repro_rate;
            let severity = '';
            let severityEmoji = '';
            if (rate >= 0.9) {
              severity = 'CRITICAL';
              severityEmoji = 'ðŸ”´';
            } else if (rate >= 0.5) {
              severity = 'HIGH';
              severityEmoji = 'ðŸŸ ';
            } else if (rate >= 0.1) {
              severity = 'MEDIUM';
              severityEmoji = 'ðŸŸ¡';
            } else if (rate > 0) {
              severity = 'LOW';
              severityEmoji = 'ðŸŸ¢';
            } else {
              severity = 'NONE';
              severityEmoji = 'âœ…';
            }

            const passed = result.total_runs - result.failures;

            let body = `## ${severityEmoji} Flaky Test Detection Results

            **Severity:** ${severity}

            ### Summary
            | Metric | Value |
            |--------|-------|
            | Total Runs | ${result.total_runs} |
            | âœ… Passed | ${passed} |
            | âŒ Failed | ${result.failures} |
            | ðŸ“Š Failure Rate | ${(rate * 100).toFixed(1)}% |

            `;

            // Add interpretation
            if (rate >= 0.9) {
              body += `### âŒ Analysis: Real Bug Detected

            This appears to be a **real bug**, not flaky behavior. The test fails ${(rate * 100).toFixed(1)}% of the time.

            **Recommendation:** Fix the underlying issue before merging.
            `;
            } else if (rate >= 0.5) {
              body += `### âš ï¸ Analysis: Severe Flakiness

            Very unstable test - fails ${(rate * 100).toFixed(1)}% of the time.

            **Recommendation:** Block this PR and fix the flaky test.
            `;
            } else if (rate >= 0.1) {
              body += `### ðŸŸ¡ Analysis: Flaky Behavior Detected

            Clear flaky behavior - test fails ${(rate * 100).toFixed(1)}% of the time.

            **Recommendation:** Should be fixed before merging.
            `;
            } else if (rate > 0) {
              body += `### ðŸŸ¢ Analysis: Low-Level Flakiness

            Test occasionally fails (${(rate * 100).toFixed(1)}% of the time) but is mostly stable.

            **Recommendation:** Monitor and consider fixing when time allows.
            `;
            } else {
              body += `### âœ… Analysis: No Flakiness Detected

            The test failure appears to be a one-time issue.

            **Recommendation:** Safe to retry or merge if tests pass on retry.
            `;
            }

            // Add changes information
            const fs = require('fs');
            if (fs.existsSync('changed_files.txt')) {
              const changedFiles = fs.readFileSync('changed_files.txt', 'utf8')
                .split('\n')
                .filter(f => f.trim());

              if (changedFiles.length > 0) {
                const pythonFiles = changedFiles.filter(f => f.endsWith('.py'));
                const coreFiles = pythonFiles.filter(f =>
                  ['worker.py', 'config.py', 'database.py'].includes(f)
                );

                body += `\n### ðŸ“ Changes That May Have Introduced Flakiness\n\n`;

                // Show commit information
                if (fs.existsSync('commits_detailed.txt')) {
                  const commits = fs.readFileSync('commits_detailed.txt', 'utf8')
                    .split('\n')
                    .filter(c => c.trim());

                  if (commits.length > 0) {
                    body += `**Commits:** ${commits.length} new commit(s)\n\n`;
                    body += '<details>\n<summary>View commits</summary>\n\n';
                    commits.slice(0, 5).forEach(commit => {
                      const parts = commit.split('|');
                      if (parts.length >= 4) {
                        const [sha, author, time, message] = parts;
                        const shortMsg = message.length > 60 ? message.substring(0, 57) + '...' : message;
                        body += `- \`${sha}\` ${shortMsg} - _${author} (${time})_\n`;
                      }
                    });
                    if (commits.length > 5) {
                      body += `- ... and ${commits.length - 5} more commits\n`;
                    }
                    body += '\n</details>\n\n';
                  }
                }

                body += `**Files Changed:** ${changedFiles.length} total`;
                if (pythonFiles.length > 0) {
                  body += `, ${pythonFiles.length} Python files`;
                }
                body += '\n\n';

                if (coreFiles.length > 0) {
                  body += `âš ï¸ **Core modules modified:** ${coreFiles.map(f => \`\\\`${f}\\\`\`).join(', ')}\n\n`;
                }

                if (pythonFiles.length > 0) {
                  body += '<details>\n<summary>View changed Python files</summary>\n\n';
                  pythonFiles.slice(0, 10).forEach(f => {
                    body += `- \`${f}\`\n`;
                  });
                  if (pythonFiles.length > 10) {
                    body += `- ... and ${pythonFiles.length - 10} more\n`;
                  }
                  body += '\n</details>\n\n';
                }
              }
            }

            // Add sample failures
            if (result.failures > 0) {
              const failedRuns = result.results.filter(r => !r.passed).slice(0, 3);
              body += `\n### Sample Failed Runs\n\n`;
              failedRuns.forEach(run => {
                const errorMsg = run.stderr || run.stdout || 'No output';
                body += `**Run #${run.attempt}:**\n\`\`\`\n${errorMsg.substring(0, 300)}\n\`\`\`\n\n`;
              });
            }

            body += `\n---\n*Detected by [Serverless Flaky Test Detector](https://github.com/${context.repo.owner}/${context.repo.repo})*`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: ${{ steps.extract-tests.outputs.pr_number }},
              body: body
            });

      - name: Send Slack notification
        if: always() && env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          GITHUB_SLACK_MAP: ${{ secrets.GITHUB_SLACK_MAP }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_SERVER_URL: ${{ github.server_url }}
        run: |
          python << 'EOF'
          import json
          import os
          import requests
          import re

          webhook_url = os.environ.get('SLACK_WEBHOOK_URL')
          if not webhook_url:
              print("No Slack webhook URL configured, skipping notification")
              exit(0)

          # Load GitHub to Slack user mapping (optional)
          # Format: {"github_username": "slack_user_id", ...}
          # Slack user IDs can be found in Slack profile (e.g., "U01234ABCD")
          user_map = {}
          map_json = os.environ.get('GITHUB_SLACK_MAP', '{}')
          try:
              user_map = json.loads(map_json)
          except:
              print("No valid GitHub-Slack user mapping found")

          # Load results
          with open('flaky_test_results.json') as f:
              result = json.load(f)

          rate = result['repro_rate']
          if rate >= 0.9:
              severity = "ðŸ”´ CRITICAL"
              color = "#d73a4a"
          elif rate >= 0.5:
              severity = "ðŸŸ  HIGH"
              color = "#fb8500"
          elif rate >= 0.1:
              severity = "ðŸŸ¡ MEDIUM"
              color = "#ffd60a"
          elif rate > 0:
              severity = "ðŸŸ¢ LOW"
              color = "#2ea44f"
          else:
              severity = "âœ… NONE"
              color = "#2ea44f"

          # Read commit information
          commits_info = []
          commit_authors = set()
          if os.path.exists('commits_detailed.txt'):
              with open('commits_detailed.txt') as cf:
                  for line in cf:
                      line = line.strip()
                      if line:
                          parts = line.split('|', 3)
                          if len(parts) >= 4:
                              sha, author, time, message = parts[0], parts[1], parts[2], parts[3]
                              commits_info.append({
                                  'sha': sha,
                                  'author': author,
                                  'time': time,
                                  'message': message[:60] + '...' if len(message) > 60 else message
                              })
                              commit_authors.add(author)

          # Build notification text with mentions
          mention_text = ""
          if commit_authors:
              mentions = []
              for author in commit_authors:
                  # Try to map GitHub username to Slack user ID
                  if author in user_map:
                      mentions.append(f"<@{user_map[author]}>")
                  else:
                      mentions.append(f"`{author}`")
              mention_text = f"FYI: {', '.join(mentions)}"

          # Build blocks
          blocks = [
              {
                  "type": "header",
                  "text": {
                      "type": "plain_text",
                      "text": f"{severity} Flaky Test Detected"
                  }
              },
              {
                  "type": "section",
                  "fields": [
                      {
                          "type": "mrkdwn",
                          "text": f"*Repository:*\n{os.environ.get('GITHUB_REPOSITORY', 'Unknown')}"
                      },
                      {
                          "type": "mrkdwn",
                          "text": f"*Failure Rate:*\n{rate * 100:.1f}%"
                      },
                      {
                          "type": "mrkdwn",
                          "text": f"*Total Runs:*\n{result['total_runs']}"
                      },
                      {
                          "type": "mrkdwn",
                          "text": f"*Failed Runs:*\n{result['failures']}"
                      }
                  ]
              }
          ]

          # Add commit information section
          if commits_info:
              commit_text = f"*Recent Commits ({len(commits_info)}):*\n"
              for commit in commits_info[:3]:
                  author_display = commit['author']
                  if commit['author'] in user_map:
                      author_display = f"<@{user_map[commit['author']]}>"
                  commit_text += f"â€¢ `{commit['sha']}` {commit['message']} - {author_display}\n"

              if len(commits_info) > 3:
                  commit_text += f"â€¢ ... and {len(commits_info) - 3} more commits\n"

              blocks.append({
                  "type": "section",
                  "text": {
                      "type": "mrkdwn",
                      "text": commit_text
                  }
              })

          # Add mention section
          if mention_text:
              blocks.append({
                  "type": "context",
                  "elements": [
                      {
                          "type": "mrkdwn",
                          "text": mention_text
                      }
                  ]
              })

          # Add action button
          github_url = f"{os.environ.get('GITHUB_SERVER_URL', 'https://github.com')}/{os.environ.get('GITHUB_REPOSITORY', '')}/actions/runs/{os.environ.get('GITHUB_RUN_ID', '')}"
          blocks.append({
              "type": "actions",
              "elements": [
                  {
                      "type": "button",
                      "text": {
                          "type": "plain_text",
                          "text": "View in GitHub Actions"
                      },
                      "url": github_url,
                      "style": "primary" if rate >= 0.5 else "default"
                  }
              ]
          })

          payload = {
              "text": f"{severity} Flaky test detected",
              "blocks": blocks,
              "attachments": [
                  {
                      "color": color,
                      "text": f"Flakiness detected with {rate * 100:.1f}% failure rate"
                  }
              ]
          }

          response = requests.post(webhook_url, json=payload)
          if response.status_code == 200:
              print("Slack notification sent successfully")
              if user_map:
                  print(f"Tagged {len(commit_authors)} commit author(s)")
          else:
              print(f"Failed to send Slack notification: {response.status_code}")
          EOF

      - name: Upload detailed results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flaky-test-results
          path: flaky_test_results.json
          retention-days: 30
