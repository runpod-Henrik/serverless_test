name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint-and-type-check:
    name: Lint and Type Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run ruff linter
        run: |
          ruff check . --output-format=github

      - name: Run ruff formatter check
        run: |
          ruff format --check .

      - name: Run mypy type checking
        run: |
          mypy worker.py config.py database.py

      - name: Generate lint and type check summary
        if: always()
        run: |
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          ## Code Quality Summary

          | Check | Status |
          |-------|--------|
          | ðŸ” Ruff Linting | âœ… Passed |
          | ðŸ“ Code Formatting | âœ… Passed |
          | ðŸ”’ Type Checking | âœ… Passed |

          All code quality checks completed successfully!
          EOF

  test:
    name: Test Suite
    runs-on: ubuntu-latest
    needs: lint-and-type-check

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Get last successful run
        id: last-success
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'ci.yml',
              branch: context.ref.replace('refs/heads/', ''),
              status: 'success',
              per_page: 1
            });

            if (runs.data.workflow_runs.length > 0) {
              const lastSuccess = runs.data.workflow_runs[0];
              core.setOutput('sha', lastSuccess.head_sha);
              core.setOutput('run_number', lastSuccess.run_number);
              core.setOutput('created_at', lastSuccess.created_at);
            } else {
              core.setOutput('sha', '');
              core.setOutput('run_number', '');
              core.setOutput('created_at', '');
            }

      - name: Run tests with coverage
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          pytest tests/test_config.py tests/test_database.py tests/test_worker.py \
            -v \
            --tb=short \
            --cov=worker \
            --cov=config \
            --cov=database \
            --cov-report=term-missing \
            --cov-report=xml \
            --cov-report=html \
            --cov-fail-under=90 \
            --junit-xml=test-results.xml

      - name: Analyze code changes
        id: changes
        if: always() && steps.last-success.outputs.sha != ''
        run: |
          LAST_SHA="${{ steps.last-success.outputs.sha }}"

          # Get changed files
          git diff --name-only $LAST_SHA HEAD > changed_files.txt

          # Get statistics
          TOTAL_CHANGED=$(wc -l < changed_files.txt | tr -d ' ')
          PYTHON_CHANGED=$(grep "\.py$" changed_files.txt | wc -l | tr -d ' ')
          WORKFLOW_CHANGED=$(grep "\.github/workflows" changed_files.txt | wc -l | tr -d ' ')

          # Get detailed diff stats
          git diff --stat $LAST_SHA HEAD > diff_stats.txt

          # Get commit information
          git log --oneline --no-decorate $LAST_SHA..HEAD > commits_oneline.txt
          git log --format="%h|%an|%ae|%ar|%s" $LAST_SHA..HEAD > commits_detailed.txt
          COMMIT_COUNT=$(git rev-list --count $LAST_SHA..HEAD)

          echo "total_changed=$TOTAL_CHANGED" >> $GITHUB_OUTPUT
          echo "python_changed=$PYTHON_CHANGED" >> $GITHUB_OUTPUT
          echo "workflow_changed=$WORKFLOW_CHANGED" >> $GITHUB_OUTPUT
          echo "last_success_sha=$LAST_SHA" >> $GITHUB_OUTPUT
          echo "commit_count=$COMMIT_COUNT" >> $GITHUB_OUTPUT

      - name: Generate test summary
        if: always()
        env:
          LAST_SUCCESS_SHA: ${{ steps.changes.outputs.last_success_sha }}
        run: |
          python << 'EOF'
          import xml.etree.ElementTree as ET
          import os
          import subprocess

          # Parse test results
          tree = ET.parse('test-results.xml')
          root = tree.getroot()

          # Extract statistics
          tests = int(root.attrib.get('tests', 0))
          failures = int(root.attrib.get('failures', 0))
          errors = int(root.attrib.get('errors', 0))
          skipped = int(root.attrib.get('skipped', 0))
          time = float(root.attrib.get('time', 0))
          passed = tests - failures - errors - skipped

          # Parse coverage from coverage.xml
          try:
              cov_tree = ET.parse('coverage.xml')
              cov_root = cov_tree.getroot()
              coverage = float(cov_root.attrib.get('line-rate', 0)) * 100
          except:
              coverage = 0.0

          # Write to GitHub summary
          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write('## Test Results Summary\n\n')

              # Overall status
              if failures + errors == 0:
                  f.write('âœ… **All tests passed!**\n\n')
              else:
                  f.write(f'âŒ **{failures + errors} test(s) failed**\n\n')

              # Show changes since last success (if available)
              last_success_sha = os.environ.get('LAST_SUCCESS_SHA', '')
              if last_success_sha and os.path.exists('changed_files.txt'):
                  f.write('### ðŸ“ Changes Since Last Successful Run\n\n')

                  # Read changed files
                  with open('changed_files.txt') as cf:
                      changed_files = [line.strip() for line in cf if line.strip()]

                  total_changed = len(changed_files)
                  python_files = [f for f in changed_files if f.endswith('.py')]
                  workflow_files = [f for f in changed_files if '.github/workflows' in f]
                  test_files = [f for f in changed_files if 'test' in f.lower() and f.endswith('.py')]

                  # Show commit information
                  if os.path.exists('commits_detailed.txt'):
                      with open('commits_detailed.txt') as cm:
                          commits = [line.strip() for line in cm if line.strip()]

                      f.write(f'**Commits:** {len(commits)} new commit(s)\n')
                      f.write(f'**Comparing:** `{last_success_sha[:7]}...HEAD`\n\n')

                      if commits:
                          f.write('**Recent Commits:**\n\n')
                          f.write('| Commit | Author | Message |\n')
                          f.write('|--------|--------|----------|\n')
                          for commit in commits[:5]:
                              parts = commit.split('|')
                              if len(parts) >= 5:
                                  sha, author, email, time, message = parts[0], parts[1], parts[2], parts[3], '|'.join(parts[4:])
                                  # Truncate long messages
                                  if len(message) > 60:
                                      message = message[:57] + '...'
                                  f.write(f'| `{sha}` | {author} | {message} |\n')
                          if len(commits) > 5:
                              f.write(f'\n*... and {len(commits) - 5} more commits*\n')
                          f.write('\n')
                  else:
                      f.write(f'**Comparing:** `{last_success_sha[:7]}...HEAD`\n\n')
                  f.write('| Category | Count |\n')
                  f.write('|----------|-------|\n')
                  f.write(f'| Total Files Changed | {total_changed} |\n')
                  f.write(f'| Python Files | {len(python_files)} |\n')
                  f.write(f'| Test Files | {len(test_files)} |\n')
                  f.write(f'| Workflow Files | {len(workflow_files)} |\n\n')

                  # Show Python files changed
                  if python_files:
                      f.write('**Python Files Changed:**\n')
                      for pf in python_files[:10]:
                          f.write(f'- `{pf}`\n')
                      if len(python_files) > 10:
                          f.write(f'- ... and {len(python_files) - 10} more\n')
                      f.write('\n')

                  # Show diff stats
                  if os.path.exists('diff_stats.txt'):
                      try:
                          with open('diff_stats.txt') as ds:
                              diff_stats = ds.read().strip()
                          if diff_stats:
                              f.write('<details>\n<summary>ðŸ“Š Detailed Diff Statistics</summary>\n\n')
                              f.write('```\n')
                              f.write(diff_stats[:1000])  # Limit size
                              f.write('\n```\n</details>\n\n')
                      except:
                          pass

                  # Identify potentially affected tests
                  if failures + errors > 0 and python_files:
                      f.write('**Potential Breaking Changes:**\n')
                      core_files = [f for f in python_files if f in ['worker.py', 'config.py', 'database.py']]
                      if core_files:
                          f.write(f'- âš ï¸ Core modules modified: {", ".join([f"`{f}`" for f in core_files])}\n')
                      if test_files:
                          f.write(f'- ðŸ§ª Test files modified: {len(test_files)} file(s)\n')
                      f.write('\n')

              # Statistics table
              f.write('| Metric | Value |\n')
              f.write('|--------|-------|\n')
              f.write(f'| Total Tests | {tests} |\n')
              f.write(f'| âœ… Passed | {passed} |\n')
              f.write(f'| âŒ Failed | {failures} |\n')
              f.write(f'| âš ï¸ Errors | {errors} |\n')
              f.write(f'| â­ï¸ Skipped | {skipped} |\n')
              f.write(f'| â±ï¸ Duration | {time:.2f}s |\n')
              f.write(f'| ðŸ“Š Coverage | {coverage:.1f}% |\n\n')

              # Failed tests details
              if failures + errors > 0:
                  f.write('### Failed Tests\n\n')
                  for testcase in root.iter('testcase'):
                      failure = testcase.find('failure')
                      error = testcase.find('error')

                      if failure is not None or error is not None:
                          classname = testcase.attrib.get('classname', '')
                          name = testcase.attrib.get('name', '')
                          test_time = testcase.attrib.get('time', '0')

                          f.write(f'#### âŒ `{classname}.{name}`\n\n')
                          f.write(f'- **Duration:** {float(test_time):.2f}s\n')

                          if failure is not None:
                              message = failure.attrib.get('message', 'No message')
                              f.write(f'- **Failure:** {message}\n')
                              if failure.text:
                                  f.write(f'\n```\n{failure.text[:500]}\n```\n\n')

                          if error is not None:
                              message = error.attrib.get('message', 'No message')
                              f.write(f'- **Error:** {message}\n')
                              if error.text:
                                  f.write(f'\n```\n{error.text[:500]}\n```\n\n')

              # Coverage status
              if coverage >= 95:
                  f.write('### ðŸŸ¢ Coverage Status: Excellent (â‰¥95%)\n')
              elif coverage >= 90:
                  f.write('### ðŸŸ¡ Coverage Status: Good (â‰¥90%)\n')
              else:
                  f.write('### ðŸ”´ Coverage Status: Needs Improvement (<90%)\n')
          EOF

      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage.xml

      - name: Upload test results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            .pytest_cache/
            test-results/

      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: py-cov-action/python-coverage-comment-action@v3
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          MINIMUM_GREEN: 95
          MINIMUM_ORANGE: 90
